

#Not a survey
you need to start with Kaggle as a motivation
Show all the methods used and their experiments results;
~Dynamic Time Warping
~Moving Average
~Linear Regression
~Linear Regression with features (Tempt etc)
~Ensemble learning

kaggle score, CV



Algorithm questions:
1. The shape of a MLR is a curve, then why we call it linear method?
2. Papers don't describe algorithms into depth
3. The distribution of holiday inbalance CV results. 
4. Other ways to use feature.

7.76% of test is holiday
3.05% of holdout data is holiday

We have a proportion about 7.5% of sample being a holiday week.
Since in the errors made in holiday examples are inflated by a 5x factor, if you measure the WMAE in samples that have different proportion of holidays, such, for example, should be the case of public leaderboard set, you will get a different WMAE, due to this different proportion.


Avg Tempture improved scored by 230. 03501 position by 8
                      
avgTemprature 4114.76384
avgTemprature + avgCPI 4585.63364
avgTemprature + avgFuelPrice 4276.63468
avgTemprature + avgFuelPrice using LARS Lasso  4563.53311

There are 78 missing records between test and train



Issues:
Ensemble learning result did not outperm single MLR
Adding new features did not improve, introducing nodisy instead
There are too many features if we take all
Ideas: moving average + seasonality index

 
# Week Three Issues
Hold out train data
result from cross validation, different window sizes
understand evaluation metrics
A really important issue is when test records is many more than train records, cannot apply DTW propper

Validation Results:
Multiple linear regression:
train window size : 20% training
    # WMAE 2056 for 5% sampling 
    # WMAE 2192 for 10% sampling
    # WMAE 2560 for 20% sampling
    # WMAE 2655 for 25% sampling
    Kaggle : # WMAE 4344.79886  <== Best in Kaggle

train window size : 10% training
    # WMAE 1899 for 5% sampling
    # WMAE 1944 for 10% sampling
    # WMAE 2346 for 20% sampling
    # WMAE 2449 for 25% sampling   <== Best in CV
    Kaggle : # WMAE 4465.69095

train window size : 6.66% training
    # WMAE 2004 for 5% sampling
    # WMAE 2007 for 10% sampling
    # WMAE 2393 for 20% sampling
    # WMAE 2453 for 25% sampling
    Kaggle : # WMAE 4520.02388

train window size : 5% training
    # WMAE 2051 for 5% sampling
    # WMAE 2065 for 10% sampling
    # WMAE 2408 for 20% sampling
    # WMAE 2489 for 25% sampling

DTW:
train window size : 25% trainingLength - forecast
    # WMAE 2468 for 25% sampling

train window size : 20% trainingLength - forecast  <== Best in CV
    # WMAE 2460 for 25% sampling
    Kaggle  # WMAE 5253.52741


train window size : 16.6% trainingLength - forecast
    # WMAE 2573 for 25% sampling

train window size : 12.5% trainingLength - forecast
    # WMAE 2746 for 25% sampling
    Kaggle  # WMAE 5255.95739

train window size : 10% trainingLength - forecast
    # WMAE 2795 for 25% sampling
    Kaggle  # WMAE 5240.86445     <== Best in Kaggle

train window size : 6.66% trainingLength - forecast
    # WMAE 2986 for 25% sampling 


Featured linear regression:
train window size : 20% training <= Kaggle 5081.83985

Essamble:
MLR + DTW <= Kaggle 4454.09373



To-Do:
When dept is not in train, find the dept from anther store

To Print:
 parts of feature.csv, store.csv to show James
 every dept's records number
 The evaluation metrics from Word?




# Materials:
Cross-validation : http://robjhyndman.com/hyndsight/crossvalidation/
DTW example: http://www.r-bloggers.com/time-series-matching-with-dynamic-time-warping/


# For thesis:
Methods:
Dynamic Time Warping: Like edit distance
Restrictions on Warping Paths:
Continuity: No elements may be skipped in a sequence (we may need some points to be skipped )


forecastDTW
Multiple Linear Regression

Exponential Smoothing:
Pros: use all the data points and have different weight for each data point
F(t+1) = αD(t) + (1-α)F(t)
samller α will make more data point contribute to the forecast
or say if we wanna all data points to contribute reasonably, α has to be reasonably small

# Week Two
After examaming the data and tried the methods, found training data is not equally distributed
Some depts have just very limited training data: 1, 77 has three records only, 1,78 has  4 records
some need more test than given training recordforecastDTW
even more, some store dept sales records don't appear in train ! such as 5, 99 9,99
problem is how to specify window size, train record are very limited in some case


# Week One
Technologies used?
This is a time series analysis and multi-step ahead prediction problem in machine learning

Methods: 
using weka's dedicated time series analysis environment which allows forecasting models to be developed, evaluated and visualized.


this differs from typical data mining/machine learning applications where each data point is an independent example of the concept to be learned, and the ordering of data points within a data set does not matter.

Weka (>= 3.7.3) 

Regression algorithms are most likely useful, or methods capable of predicting continuous target can be applied 
dependency between the past and the future
Purely random processes
Random walk

a problem of supervised learning problem, where we have to infer from historical data the possibly nonlinear dependance between the input (past embedding vector) and the output (future value).
one-step-ahead prediction



################################
We don't have a fixed length for each dept sales data, for hold out:
    it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training
In traditional forecasting of economic data, the analyzed series mainly represent yearly, quarterly, or monthly acquired data, so that series hardly reach a length longer than a couple of hundreds of values.

For evaluation, usually a part at the end of each series is reserved and not used during model generation. This is often called out-of-sample evaluation [47]. To avoid confusion with other evaluation techniques, we will call validation using a set taken from the end of the series last block validation.

 Methods widely used are linear methods (made popular by Box and Jenkins [8]) such as autoregression (AR), moving average (MA), or combinations of these (ARMA) Based on these methods, various approaches exist to tackle non-stationarity in the series. A possibility is to use the derivative of the series as input (ARIMA). Or seasonal decomposition procedures, e.g., the STL procedure [16], are used to decompose the series into (deterministic) trend and seasonality, and a stochastic process of the residuals that is stationary.



 Kaggle Abstract:
 I read a lot about CV and understand that most people mean it as a means of checking training set against LB. I learnt and will use it for determining which model is better (defining models in some various ways) CV (as in regularisation). As the data for this competition is time-series this can be done in many ways (truncating, adding features etc...). Point is that this competition, as well as the ASUS and the Loan Default deals with future data points to predict. It is not possible in my mind to have pure CV in such cases, simply because the future changes and that data is not predictable as compared to the training set. In economy it is very obvious since market rates changes. And in the competitions as well the definition of the test set is different. At the same time it feels wrong to validate my model against the LB simply because what if I end up having to take care of the analysis of an important company with data that only I have? Then it is up to me to do the best prediction, with no LB to compare with. So I rather do CV or training set CV to get to some statistical good measure of the future rather than 'checking with the LB'. Someone out there understanding my point? (all in all i like the whole concept of Kaggle and the metric but in the end it is only at Kaggle I can compare results and in real life things are different, ok someone can look at my solutions using CV etc but then i have to look good hopefully...)